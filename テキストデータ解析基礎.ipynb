{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'go', 'head', 'in', 'is', 'not', 'out', 'rain', 'raining', 'shining', 'sun', 'the']\n",
      "[[0 0 0 0 1 0 0 0 0 1 1 1]\n",
      " [0 0 1 1 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 1 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "docs = [\n",
    "    'The sun is shining',\n",
    "    'In is raining head',\n",
    "    'Do not go out in the rain'\n",
    "]\n",
    "\n",
    "\n",
    "bag = count_vec.fit_transform(docs)\n",
    "print(count_vec.get_feature_names())\n",
    "\n",
    "print(bag.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'go', 'head', 'in', 'is', 'not', 'out', 'rain', 'raining', 'shining', 'sun', 'the']\n",
      "[[ 0.          0.          0.          0.          0.42804604  0.          0.\n",
      "   0.          0.          0.5628291   0.5628291   0.42804604]\n",
      " [ 0.          0.          0.5628291   0.42804604  0.42804604  0.          0.\n",
      "   0.          0.5628291   0.          0.          0.        ]\n",
      " [ 0.40301621  0.40301621  0.          0.30650422  0.          0.40301621\n",
      "   0.40301621  0.40301621  0.          0.          0.          0.30650422]]\n"
     ]
    }
   ],
   "source": [
    "# TD-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "docs = [\n",
    "    'The sun is shining',\n",
    "    'In is raining head',\n",
    "    'Do not go out in the rain'\n",
    "]\n",
    "\n",
    "bag = tfidf_vec.fit_transform(docs)\n",
    "\n",
    "print(tfidf_vec.get_feature_names())\n",
    "\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'and', 'any', 'applications', 'basic', 'book', 'courses', 'crawling', 'data', 'day', 'every', 'for', 'format', 'from', 'graduate', 'in', 'intended', 'introductory', 'is', 'learn', 'learning', 'level', 'machine', 'mechanics', 'methods', 'of', 'range', 'scrape', 'scraping', 'several', 'source', 'storing', 'support', 'teaches', 'techniques', 'the', 'to', 'today', 'undergraduate', 'underlies', 'unlimited', 'upper', 'use', 'we', 'web', 'you']\n",
      "[[ 0.23898318  0.19281012  0.47796636  0.          0.          0.          0.\n",
      "   0.23898318  0.19281012  0.          0.          0.          0.23898318\n",
      "   0.23898318  0.          0.19281012  0.          0.          0.\n",
      "   0.19281012  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.19281012  0.          0.23898318  0.          0.\n",
      "   0.          0.23898318  0.          0.19281012  0.          0.          0.\n",
      "   0.23898318  0.          0.          0.          0.38562025  0.        ]\n",
      " [ 0.          0.          0.          0.          0.48214012  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.48214012  0.          0.          0.          0.\n",
      "   0.38898761  0.          0.          0.          0.          0.48214012\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.38898761  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.28609079  0.          0.          0.35460217  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.28609079\n",
      "   0.          0.          0.          0.          0.35460217  0.          0.\n",
      "   0.35460217  0.          0.35460217  0.          0.35460217  0.          0.\n",
      "   0.          0.28609079  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.35460217]\n",
      " [ 0.          0.19622238  0.          0.          0.          0.24321258\n",
      "   0.24321258  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.24321258  0.19622238  0.24321258  0.24321258  0.24321258\n",
      "   0.          0.19622238  0.48642516  0.19622238  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.24321258  0.          0.          0.19622238  0.19622238  0.\n",
      "   0.24321258  0.          0.          0.24321258  0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.31156077  0.          0.          0.\n",
      "   0.          0.          0.31156077  0.31156077  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.25136527  0.          0.25136527  0.          0.          0.31156077\n",
      "   0.31156077  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.31156077  0.\n",
      "   0.31156077  0.          0.          0.31156077  0.31156077  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "url = 'http://www.lighthouse-w5.com/python/data/testdata.zip'\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "docs = []\n",
    "\n",
    "res = requests.get(url, stream=True)\n",
    "if res.status_code == 200:\n",
    "    with open('testdata.zip', 'wb') as file:\n",
    "        for chunk in res.iter_content(chunk_size = 2014):\n",
    "                file.write(chunk)\n",
    "    \n",
    "    \n",
    "    with zipfile.ZipFile('testdata.zip', 'r') as myzip:\n",
    "        for info in myzip.infolist():\n",
    "            #print(info.filename)\n",
    "            \n",
    "            with myzip.open(info.filename) as myfile:\n",
    "                rawtext = myfile.read()\n",
    "                text = rawtext.decode('shift_jis')\n",
    "                docs.append(text)\n",
    "\n",
    "    bag = tfidf_vec.fit_transform(docs)\n",
    "\n",
    "    print(tfidf_vec.get_feature_names())\n",
    "\n",
    "    print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head', 'rain', 'raining', 'shining', 'sun']\n",
      "[[0 0 0 1 1]\n",
      " [1 0 1 0 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(stop_words = 'english')\n",
    "docs = [\n",
    "    'The sun is shining',\n",
    "    'In is raining head',\n",
    "    'Do not go out in the rain'\n",
    "]\n",
    "\n",
    "\n",
    "bag = count_vec.fit_transform(docs)\n",
    "print(count_vec.get_feature_names())\n",
    "\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head', 'rain', 'shine', 'sun']\n",
      "[[0 0 1 1]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectornizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc : (stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "docs = [\n",
    "    'The sun is shining',\n",
    "    'In is raining head',\n",
    "    'Do not go out in the rain'\n",
    "]\n",
    "\n",
    "count_vec = StemmedCountVectornizer(stop_words = 'english')\n",
    "bag = count_vec.fit_transform(docs)\n",
    "print(count_vec.get_feature_names())\n",
    "\n",
    "print(bag.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'applic', 'basic', 'book', 'cours', 'crawl', 'data', 'day', 'format', 'graduat', 'intend', 'introductori', 'learn', 'level', 'machin', 'mechan', 'method', 'rang', 'scrape', 'sourc', 'store', 'support', 'teach', 'techniqu', 'today', 'under', 'undergradu', 'unlimit', 'upper', 'use', 'web']\n",
      "[[ 0.31590422  0.          0.          0.          0.          0.31590422\n",
      "   0.25486954  0.          0.31590422  0.          0.          0.\n",
      "   0.17797493  0.          0.          0.          0.          0.\n",
      "   0.21156474  0.31590422  0.          0.          0.          0.31590422\n",
      "   0.          0.          0.          0.31590422  0.          0.\n",
      "   0.50973908]\n",
      " [ 0.          0.          0.49389914  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.49389914  0.          0.          0.33077001  0.          0.\n",
      "   0.          0.49389914  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.39847472]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.43646642  0.          0.          0.          0.          0.\n",
      "   0.3047837   0.          0.          0.          0.54098887  0.\n",
      "   0.36230655  0.          0.54098887  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.2776887   0.2776887   0.          0.\n",
      "   0.          0.          0.2776887   0.2776887   0.2776887   0.15644497\n",
      "   0.5553774   0.2240375   0.          0.          0.          0.          0.\n",
      "   0.          0.2776887   0.          0.          0.          0.\n",
      "   0.2776887   0.          0.2776887   0.          0.        ]\n",
      " [ 0.          0.37882278  0.          0.          0.          0.          0.\n",
      "   0.37882278  0.          0.          0.          0.          0.21342214\n",
      "   0.          0.30563183  0.          0.          0.37882278  0.          0.\n",
      "   0.          0.          0.          0.          0.37882278  0.37882278\n",
      "   0.          0.          0.          0.37882278  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectornizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc : (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "url = 'http://www.lighthouse-w5.com/python/data/testdata.zip'\n",
    "\n",
    "tfidf_vec = StemmedTfidfVectornizer(stop_words = 'english')\n",
    "docs = []\n",
    "\n",
    "res = requests.get(url, stream=True)\n",
    "if res.status_code == 200:\n",
    "    with open('testdata.zip', 'wb') as file:\n",
    "        for chunk in res.iter_content(chunk_size = 2014):\n",
    "                file.write(chunk)\n",
    "    \n",
    "    \n",
    "    with zipfile.ZipFile('testdata.zip', 'r') as myzip:\n",
    "        for info in myzip.infolist():\n",
    "            #print(info.filename)\n",
    "            \n",
    "            with myzip.open(info.filename) as myfile:\n",
    "                rawtext = myfile.read()\n",
    "                text = rawtext.decode('shift_jis')\n",
    "                docs.append(text)\n",
    "\n",
    "    bag = tfidf_vec.fit_transform(docs)\n",
    "\n",
    "    print(tfidf_vec.get_feature_names())\n",
    "\n",
    "    print(bag.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第\t接頭詞,数接続,*,*,*,*,第,ダイ,ダイ\n",
      "９\t名詞,数,*,*,*,*,９,キュウ,キュー\n",
      "９\t名詞,数,*,*,*,*,９,キュウ,キュー\n",
      "回\t名詞,接尾,助数詞,*,*,*,回,カイ,カイ\n",
      "全国\t名詞,一般,*,*,*,*,全国,ゼンコク,ゼンコク\n",
      "高校\t名詞,一般,*,*,*,*,高校,コウコウ,コーコー\n",
      "野球\t名詞,一般,*,*,*,*,野球,ヤキュウ,ヤキュー\n",
      "選手権\t名詞,一般,*,*,*,*,選手権,センシュケン,センシュケン\n",
      "大会\t名詞,一般,*,*,*,*,大会,タイカイ,タイカイ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "第\t接頭詞,数接続,*,*,*,*,第,ダイ,ダイ\n",
      "４\t名詞,数,*,*,*,*,４,ヨン,ヨン\n",
      "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "１\t名詞,数,*,*,*,*,１,イチ,イチ\n",
      "１\t名詞,数,*,*,*,*,１,イチ,イチ\n",
      "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "初戦\t名詞,一般,*,*,*,*,初戦,ショセン,ショセン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "迎える\t動詞,自立,*,*,一段,基本形,迎える,ムカエル,ムカエル\n",
      "横浜\t名詞,固有名詞,地域,一般,*,*,横浜,ヨコハマ,ヨコハマ\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "神奈川\t名詞,固有名詞,地域,一般,*,*,神奈川,カナガワ,カナガワ\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "増田\t名詞,固有名詞,人名,姓,*,*,増田,マスダ,マスダ\n",
      "珠\t名詞,一般,*,*,*,*,珠,タマ,タマ\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "しゅう\t名詞,一般,*,*,*,*,しゅう,シュウ,シュー\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "中堅\t名詞,一般,*,*,*,*,中堅,チュウケン,チューケン\n",
      "手\t名詞,接尾,一般,*,*,*,手,シュ,シュ\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "３\t名詞,数,*,*,*,*,３,サン,サン\n",
      "年\t名詞,接尾,助数詞,*,*,*,年,ネン,ネン\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "９\t名詞,数,*,*,*,*,９,キュウ,キュー\n",
      "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "午前\t名詞,副詞可能,*,*,*,*,午前,ゴゼン,ゴゼン\n",
      "１\t名詞,数,*,*,*,*,１,イチ,イチ\n",
      "１\t名詞,数,*,*,*,*,１,イチ,イチ\n",
      "時\t名詞,接尾,助数詞,*,*,*,時,ジ,ジ\n",
      "２\t名詞,数,*,*,*,*,２,ニ,ニ\n",
      "分\t名詞,接尾,助数詞,*,*,*,分,フン,フン\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "兵庫\t名詞,固有名詞,地域,一般,*,*,兵庫,ヒョウゴ,ヒョーゴ\n",
      "県\t名詞,接尾,地域,*,*,*,県,ケン,ケン\n",
      "伊丹\t名詞,固有名詞,地域,一般,*,*,伊丹,イタミ,イタミ\n",
      "市\t名詞,接尾,地域,*,*,*,市,シ,シ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "練習\t名詞,サ変接続,*,*,*,*,練習,レンシュウ,レンシュー\n",
      "場\t名詞,接尾,一般,*,*,*,場,ジョウ,ジョー\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "いつも\t副詞,一般,*,*,*,*,いつも,イツモ,イツモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "年\t名詞,一般,*,*,*,*,年,トシ,トシ\n",
      "と\t助詞,格助詞,一般,*,*,*,と,ト,ト\n",
      "同じ\t連体詞,*,*,*,*,*,同じ,オナジ,オナジ\n",
      "よう\t名詞,非自立,助動詞語幹,*,*,*,よう,ヨウ,ヨー\n",
      "に\t助詞,副詞化,*,*,*,*,に,ニ,ニ\n",
      "黙とう\t名詞,サ変接続,*,*,*,*,黙とう,モクトウ,モクトー\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "ささげる\t動詞,自立,*,*,一段,基本形,ささげる,ササゲル,ササゲル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "自身\t名詞,一般,*,*,*,*,自身,ジシン,ジシン\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "長崎\t名詞,固有名詞,地域,一般,*,*,長崎,ナガサキ,ナガサキ\n",
      "市\t名詞,接尾,地域,*,*,*,市,シ,シ\n",
      "出身\t名詞,一般,*,*,*,*,出身,シュッシン,シュッシン\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "祖母\t名詞,一般,*,*,*,*,祖母,ソボ,ソボ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "久美子\t名詞,固有名詞,人名,名,*,*,久美子,クミコ,クミコ\n",
      "さん\t名詞,接尾,人名,*,*,*,さん,サン,サン\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "７\t名詞,数,*,*,*,*,７,ナナ,ナナ\n",
      "４\t名詞,数,*,*,*,*,４,ヨン,ヨン\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "広島\t名詞,固有名詞,地域,一般,*,*,広島,ヒロシマ,ヒロシマ\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "被爆\t名詞,サ変接続,*,*,*,*,被爆,ヒバク,ヒバク\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "出身\t名詞,一般,*,*,*,*,出身,シュッシン,シュッシン\n",
      "地\t名詞,接尾,一般,*,*,*,地,チ,チ\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "関東\t名詞,固有名詞,地域,一般,*,*,関東,カントウ,カントー\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\n",
      "原爆\t名詞,一般,*,*,*,*,原爆,ゲンバク,ゲンバク\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "日\t名詞,非自立,副詞可能,*,*,*,日,ヒ,ヒ\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\n",
      "へ\t助詞,格助詞,一般,*,*,*,へ,ヘ,エ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "意識\t名詞,サ変接続,*,*,*,*,意識,イシキ,イシキ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "差\t名詞,一般,*,*,*,*,差,サ,サ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "戸惑い\t動詞,自立,*,*,五段・ワ行促音便,連用形,戸惑う,トマドイ,トマドイ\n",
      "ながら\t助詞,接続助詞,*,*,*,*,ながら,ナガラ,ナガラ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\n",
      "長崎\t名詞,固有名詞,地域,一般,*,*,長崎,ナガサキ,ナガサキ\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "起き\t動詞,自立,*,*,一段,連用形,起きる,オキ,オキ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "こと\t名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "伝え\t動詞,自立,*,*,一段,連用形,伝える,ツタエ,ツタエ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いか\t動詞,非自立,*,*,五段・カ行促音便,未然形,いく,イカ,イカ\n",
      "なく\t助動詞,*,*,*,特殊・ナイ,連用テ接続,ない,ナク,ナク\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "甲子園\t名詞,固有名詞,地域,一般,*,*,甲子園,コウシエン,コーシエン\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "思い\t名詞,一般,*,*,*,*,思い,オモイ,オモイ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "新た\t名詞,形容動詞語幹,*,*,*,*,新た,アラタ,アラタ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いる\t動詞,非自立,*,*,一段,基本形,いる,イル,イル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "(\t名詞,サ変接続,*,*,*,*,(,*,*\n",
      "毎日新聞\t名詞,固有名詞,組織,*,*,*,毎日新聞,マイニチシンブン,マイニチシンブン\n",
      ")\t名詞,サ変接続,*,*,*,*,),*,*\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "token = t.tokenize('第９９回全国高校野球選手権大会の第４日（１１日）に初戦を迎える横浜（神奈川）の増田珠（しゅう）中堅手（３年）は９日の午前１１時２分、兵庫県伊丹市の練習場で、いつもの年と同じように黙とうをささげる。自身は長崎市出身で、祖母の久美子さん（７４）は広島で被爆。出身地と関東での「原爆の日」への意識の差に戸惑いながら、「長崎で起きたことを伝えていかなくては」と甲子園で思いを新たにしている。(毎日新聞)')\n",
    "for n in token:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['。', 'から', 'た', 'です', 'とても', 'の', 'は', '今日', '台', '天気', '展望', '彼女', '景色', '的', '美しかっ', '美しく', '良い', '魅力']\n",
      "[[1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def extract_words(text):\n",
    "    t = Tokenizer()\n",
    "    return [w.surface for w in t.tokenize(text)]\n",
    "\n",
    "jvectorizer = CountVectorizer(analyzer=extract_words)\n",
    "\n",
    "docs = ['今日はとても良い天気です。',\n",
    "       '彼女は美しく魅力的です。',\n",
    "       '展望台からの景色は美しかった。']\n",
    "\n",
    "bag = jvectorizer.fit_transform(docs)\n",
    "\n",
    "print(jvectorizer.get_feature_names())\n",
    "\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['いる', 'さ', 'れ', '事項', '内容', '夜', '時', '来', '検討', '記載', '資料', '降っ', '降り', '雨', '雪']\n",
      "[[0 1 0 0 0 1 0 0 0 0 0 0 1 0 1]\n",
      " [1 1 1 1 1 0 0 0 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def extract_words2(text):\n",
    "    t = Tokenizer()\n",
    "    token = t.tokenize(text)\n",
    "    words = []\n",
    "    for word in token:\n",
    "        if(word.part_of_speech.find('名詞') >= 0 or\n",
    "          word.part_of_speech.find('動詞') == 0):\n",
    "            words.append(word.surface)\n",
    "    return words\n",
    "\n",
    "docs = [\n",
    "    '雪が降り寒さが厳しい夜だ',\n",
    "    '資料には検討事項などの内容が記載されている・',\n",
    "    'その時、雨が降って来た'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=extract_words2)\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['いる', 'さ', 'する', 'れる', '事項', '内容', '夜', '時', '来る', '検討', '記載', '資料', '降る', '雨', '雪']\n",
      "[[0 1 0 0 0 0 1 0 0 0 0 0 1 0 1]\n",
      " [1 0 1 1 1 1 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def extract_words3(text):\n",
    "    t = Tokenizer()\n",
    "    token = t.tokenize(text)\n",
    "    words = []\n",
    "    for word in token:\n",
    "        if word.part_of_speech.find('名詞') >= 0 :\n",
    "            words.append(word.surface)\n",
    "        elif word.part_of_speech.find('動詞') == 0:\n",
    "            words.append(word.base_form)\n",
    "    return words\n",
    "\n",
    "docs = [\n",
    "    '雪が降り寒さが厳しい夜だ',\n",
    "    '資料には検討事項などの内容が記載されている・',\n",
    "    'その時、雨が降って来た'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=extract_words2)\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "789_ruby_5639.zip\n",
      "<ZipInfo filename='wagahaiwa_nekodearu.txt' compress_type=deflate external_attr=0x20 file_size=748959 compress_size=350260>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>する</td>\n",
       "      <td>4127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>いる</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>の</td>\n",
       "      <td>1645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>云う</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>事</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>なる</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>ある</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>もの</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>主人</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>君</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>ん</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>よう</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10529</th>\n",
       "      <td>見る</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>人</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6422</th>\n",
       "      <td>御</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>一</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>何</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6587</th>\n",
       "      <td>思う</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>れる</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>吾輩</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7731</th>\n",
       "      <td>来る</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>これ</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>それ</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>聞く</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>時</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400</th>\n",
       "      <td>行く</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>上</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>出る</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4219</th>\n",
       "      <td>出来る</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3935</th>\n",
       "      <td>傍点</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10667</th>\n",
       "      <td>記</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11420</th>\n",
       "      <td>都合</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6435</th>\n",
       "      <td>御存じ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>捲る</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7220</th>\n",
       "      <td>攻撃</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>放つ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>箱</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11562</th>\n",
       "      <td>鐘</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11564</th>\n",
       "      <td>鑑定</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>わがまま</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>罹る</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>あな</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9849</th>\n",
       "      <td>義理</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>並ぶ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>忙</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6570</th>\n",
       "      <td>怒鳴る</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>修業</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861</th>\n",
       "      <td>信用</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>依頼</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>思</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>使</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>肝心</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>肝</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>すべ</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3506</th>\n",
       "      <td>二絃琴</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>之</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>久し振り</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11276</th>\n",
       "      <td>逢</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6742</th>\n",
       "      <td>感ずる</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>一種</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1601 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  count\n",
       "1295     する   4127\n",
       "320      いる   2020\n",
       "1797      の   1645\n",
       "3509     云う   1399\n",
       "3486      事   1205\n",
       "1743     なる   1157\n",
       "209      ある   1099\n",
       "2396     もの   1004\n",
       "3416     主人    934\n",
       "4858      君    905\n",
       "2713      ん    784\n",
       "2605     よう    723\n",
       "10529    見る    686\n",
       "3544      人    601\n",
       "6422      御    566\n",
       "3067      一    561\n",
       "3767      何    542\n",
       "6587     思う    502\n",
       "2670     れる    499\n",
       "4897     吾輩    483\n",
       "7731     来る    459\n",
       "969      これ    426\n",
       "1385     それ    384\n",
       "9911     聞く    349\n",
       "7477      時    345\n",
       "10400    行く    334\n",
       "3219      上    327\n",
       "4198     出る    327\n",
       "4219    出来る    324\n",
       "3935     傍点    319\n",
       "...     ...    ...\n",
       "10667     記     10\n",
       "11420    都合     10\n",
       "6435    御存じ     10\n",
       "7102     捲る     10\n",
       "7220     攻撃     10\n",
       "7222     放つ     10\n",
       "9579      箱     10\n",
       "11562     鐘     10\n",
       "11564    鑑定     10\n",
       "2689   わがまま     10\n",
       "9819     罹る     10\n",
       "180      あな     10\n",
       "9849     義理     10\n",
       "3384     並ぶ     10\n",
       "6540      忙     10\n",
       "6570    怒鳴る     10\n",
       "3865     修業     10\n",
       "3861     信用     10\n",
       "3824     依頼     10\n",
       "6571      思     10\n",
       "3803      使     10\n",
       "9936     肝心     10\n",
       "9935      肝     10\n",
       "1280     すべ     10\n",
       "3506    二絃琴     10\n",
       "3429      之     10\n",
       "3426   久し振り     10\n",
       "11276     逢     10\n",
       "6742    感ずる     10\n",
       "3141     一種     10\n",
       "\n",
       "[1601 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os.path\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://www.aozora.gr.jp/cards/000148/files/789_ruby_5639.zip'\n",
    "data = requests.get(url)\n",
    "\n",
    "def download_file(url):\n",
    "    filename = url.rsplit('/', 1)[1].split('?')[0]\n",
    "    print(os.path.exists(filename))\n",
    "    if not os.path.exists(filename):\n",
    "        res = requests.get(url, stream=True)\n",
    "        if res.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                for chunk in res.iter_content(chunk_size = 1024):\n",
    "                    file.write(chunk)\n",
    "    return filename\n",
    "\n",
    "def extract_words3(text):\n",
    "    t = Tokenizer()\n",
    "    token = t.tokenize(text)\n",
    "    words = []\n",
    "    for word in token:\n",
    "        if word.part_of_speech.find('名詞') >= 0 :\n",
    "            words.append(word.surface)\n",
    "        elif word.part_of_speech.find('動詞') == 0:\n",
    "            words.append(word.base_form)\n",
    "    return words\n",
    "\n",
    "texts = []\n",
    "filename = download_file(url)\n",
    "print(filename)\n",
    "with ZipFile(filename, 'r') as dlzip:\n",
    "    for info in dlzip.infolist():\n",
    "       with dlzip.open (info.filename) as file:\n",
    "        print(info)\n",
    "        raw_text = file.read()\n",
    "        txt = raw_text.decode('shift_jis')\n",
    "        texts.append(txt)\n",
    "        \n",
    "vectorizer = CountVectorizer(analyzer=extract_words3)\n",
    "bag = vectorizer.fit_transform(texts)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(bag)\n",
    "\n",
    "data = pd.DataFrame(bag.toarray())\n",
    "\n",
    "data = data.T\n",
    "words = vectorizer.get_feature_names()\n",
    "data['word'] = words\n",
    "data.columns = ['count','word']\n",
    "data.loc[data['count'] >= 10, ['word','count']].sort_values(by='count', ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
